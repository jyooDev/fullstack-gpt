{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval Augmented Generation)\n",
    "## Definition\n",
    "- **NicoìŒ¤**: A technique using all retrieving data from some private sources, we can increase the capacity of the LLM.\n",
    "- **ChatGPT**: A method where a language model first looks up relevant information from a large database and then uses that information to generate more accurate and context-aware responses to user queries. It combines the strengths of both searching for facts and creating natural-sounding text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnstructuredFiledLoader\n",
    "can download any format of files\n",
    "\n",
    "### Tiktoken\n",
    "`tiktoken` is a fast **BPE** tokeniser for use with OpenAI's models.\n",
    "- **[Byte Pair Encoding](https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.CharacterTextSplitter.html#langchain_text_splitters.character.CharacterTextSplitter.from_tiktoken_encoder)** is an algorithm, first described in 1994 by Philip Gage for encoding strings of text into tabular form for use in downstream modeling. Its modification is notable as the large language model tokenizer with an ability to combine both tokens that encode single characters (including single digits or single punctuation marks) and those that encode whole words (even the longest compound words). This modification, in the first step, assumes all unique characters to be an initial set of 1-character long n-grams (i.e. initial \"tokens\"). Then, successively the most frequent pair of adjacent characters is merged into a new, 2-character long n-gram and all instances of the pair are replaced by this new token. This is repeated until a vocabulary of prescribed size is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     separators=\"\\n\",\n",
    "#     chunk_overlap = 50,\n",
    "#     chunk_size=200\n",
    "# )\n",
    "\n",
    "# from_tiktoken_encoder: splitting the text as what the model would treat the text messages is a better practice. \n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_overlap = 50,\n",
    "    chunk_size=200\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"files/example.md\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#No caching\n",
    "embedder = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_documents(docs, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vectorstore.similarity_search(\"How can i install TechGuide?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "#Caching then store in vectorstore\n",
    "cache_dir = LocalFileStore(\"../.cache/\")\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedder, cache_dir\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, cached_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"- Processor: Intel i5 or equivalent\\n- Memory: 8GB RAM\\n- Storage: 50GB free disk space\\n- Network: Stable internet connection\\nInstallation\\nDownload the Installer: Visit the official TechGuide website and download the appropriate installer for your operating system.\\nRun the Installer: Double-click the downloaded file and follow the on-screen instructions to install TechGuide on your machine.\\nFirst-Time Setup: After installation, launch TechGuide. You will be prompted to create an account or log in with existing credentials. Follow the setup wizard to configure your initial settings.\\nConfiguration\\nOnce TechGuide is installed, you need to configure it to connect with your infrastructure. Follow these steps:\\n1. Cloud Provider Integration: Navigate to the 'Integrations' section in the settings menu. Here, you can connect your AWS, Azure, or Google Cloud accounts by entering your credentials.\", metadata={'source': 'files/example.md'}), Document(page_content=\"- Processor: Intel i5 or equivalent\\n- Memory: 8GB RAM\\n- Storage: 50GB free disk space\\n- Network: Stable internet connection\\nInstallation\\nDownload the Installer: Visit the official TechGuide website and download the appropriate installer for your operating system.\\nRun the Installer: Double-click the downloaded file and follow the on-screen instructions to install TechGuide on your machine.\\nFirst-Time Setup: After installation, launch TechGuide. You will be prompted to create an account or log in with existing credentials. Follow the setup wizard to configure your initial settings.\\nConfiguration\\nOnce TechGuide is installed, you need to configure it to connect with your infrastructure. Follow these steps:\\n1. Cloud Provider Integration: Navigate to the 'Integrations' section in the settings menu. Here, you can connect your AWS, Azure, or Google Cloud accounts by entering your credentials.\", metadata={'source': 'files/example.md'}), Document(page_content=\"Introduction\\nWelcome to the TechGuide Documentation. This document provides a comprehensive guide to using the TechGuide platform, a cutting-edge solution for managing and automating your tech stack. Whether you are a developer, system administrator, or tech enthusiast, this guide will help you get the most out of TechGuide's powerful features.\\nTechGuide is designed to simplify the management of complex infrastructure, offering seamless integration with various cloud providers, robust automation capabilities, and an intuitive interface. This documentation will walk you through the setup process, highlight key features, and provide detailed instructions on using the platform effectively.\\nSetup\\nSystem Requirements\\nBefore installing TechGuide, ensure your system meets the following minimum requirements:\\n- Operating System: Windows 10, macOS 10.15+, Linux (Ubuntu 18.04+)\\n- Processor: Intel i5 or equivalent\\n- Memory: 8GB RAM\\n- Storage: 50GB free disk space\\n- Network: Stable internet connection\\nInstallation\", metadata={'source': 'files/example.md'}), Document(page_content=\"Introduction\\nWelcome to the TechGuide Documentation. This document provides a comprehensive guide to using the TechGuide platform, a cutting-edge solution for managing and automating your tech stack. Whether you are a developer, system administrator, or tech enthusiast, this guide will help you get the most out of TechGuide's powerful features.\\nTechGuide is designed to simplify the management of complex infrastructure, offering seamless integration with various cloud providers, robust automation capabilities, and an intuitive interface. This documentation will walk you through the setup process, highlight key features, and provide detailed instructions on using the platform effectively.\\nSetup\\nSystem Requirements\\nBefore installing TechGuide, ensure your system meets the following minimum requirements:\\n- Operating System: Windows 10, macOS 10.15+, Linux (Ubuntu 18.04+)\\n- Processor: Intel i5 or equivalent\\n- Memory: 8GB RAM\\n- Storage: 50GB free disk space\\n- Network: Stable internet connection\\nInstallation\", metadata={'source': 'files/example.md'})]\n"
     ]
    }
   ],
   "source": [
    "results = vectorstore.similarity_search(\"How can i install TechGuide?\")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "### LangChain Expression Language (LCEL)\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores, but are also broad enough to include Wikipedia search and Amazon Kendra.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Document's as output.\n",
    "\n",
    "** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnablePathThrough\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chain = {\"context\":retriever, \"question\": RunnablePathThrough() } | prompt | llm\n",
    "\n",
    "chain.invoke(question = \"How can i install TechGuide?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
