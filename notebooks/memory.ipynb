{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Buffer Memory\n",
    "> Simply store all previous conversations.\n",
    "\n",
    "**Cons**: The memory keeps growing as it will store every conversation with User and API. This is neither sustainable nor cose-effective.\n",
    "as you keep running `save_context` function, the result history will ever get growing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='Hi'), AIMessage(content='How are you?')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversation Buffer Memory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Regardless of memory type, they use the same API. \n",
    "memory = ConversationBufferMemory(return_messages=True) #turn return_message=False if it is not for chat model\n",
    "memory.save_context({\"input\": \"Hi\"},{\"output\": \"How are you?\"})\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Buffer Window Memory\n",
    "> It holdes the input message up to the point. For high level explanation, this memory will hold the most recent message up to the certain side you specified.\n",
    "\n",
    "**Cons**: The chat bot will only remember the recent conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='6'),\n",
       "  AIMessage(content='6'),\n",
       "  HumanMessage(content='7'),\n",
       "  AIMessage(content='7'),\n",
       "  HumanMessage(content='8'),\n",
       "  AIMessage(content='8'),\n",
       "  HumanMessage(content='9'),\n",
       "  AIMessage(content='9')]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import conversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(return_messages=True, k=4) #k = num of recent conversations to save\n",
    "\n",
    "for i in range(0,10):\n",
    "    memory.save_context({\"input\": i}, {\"output\": i})\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory\n",
    "> creats a summary of the conversation over time. Advantages of using this type of memory kicks in especiallly when keeping longer conversations in verbatim would take up too many tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content=\"The human expresses a craving for something sweet, and the AI shares that it feels the same way and asks what the human wants. The human mentions they will probably go get ice cream and invites the AI to tag along, to which the AI responds that it's a good idea. The human reveals that strawberry cheesecake is their favorite flavor, while the AI states that it prefers coconut mango.\")]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    "    model = 'gpt-4o-mini',\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=llm, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"I am craving something sweet!\", \"Same here. What do you want?\")\n",
    "add_message(\"I will probably go get ice cream. Do you want to tag along?\", \"That's a good idea.\")\n",
    "add_message(\"Strawberry cheesecake is my favorite flavor\", \"Well, I like coconut mango more.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryBufferMemory\n",
    "> It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both. It uses token length rather than number of interactions to determine when to flush interactions.\n",
    "\n",
    "### Notes:\n",
    "model = 'gpt-4o-mini' is not supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='I am craving something sweet!'),\n",
       "  AIMessage(content='Same here. What do you want?'),\n",
       "  HumanMessage(content='I will probably go get ice cream. Do you want to tag along?'),\n",
       "  AIMessage(content=\"That's a good idea.\"),\n",
       "  HumanMessage(content='Strawberry cheesecake is my favorite flavor'),\n",
       "  AIMessage(content='Well, I like coconut mango more.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature = 0.1,\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=150, return_messages=True)\n",
    "\n",
    "def add_message(input, output):\n",
    "    memory.save_context({\"input\": input}, {\"output\": output})\n",
    "\n",
    "add_message(\"I am craving something sweet!\", \"Same here. What do you want?\")\n",
    "add_message(\"I will probably go get ice cream. Do you want to tag along?\", \"That's a good idea.\")\n",
    "add_message(\"Strawberry cheesecake is my favorite flavor\", \"Well, I like coconut mango more.\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human expresses a craving for something sweet, to which the AI responds that it also wants something sweet and asks the human what they desire.'),\n",
       "  HumanMessage(content='I will probably go get ice cream. Do you want to tag along?'),\n",
       "  AIMessage(content=\"That's a good idea.\"),\n",
       "  HumanMessage(content='Strawberry cheesecake is my favorite flavor'),\n",
       "  AIMessage(content='Well, I like coconut mango more.'),\n",
       "  HumanMessage(content='Strawberry cheesecake is my favorite flavor'),\n",
       "  AIMessage(content='Well, I like coconut mango more.'),\n",
       "  HumanMessage(content='Strawberry cheesecake is my favorite flavor'),\n",
       "  AIMessage(content='Well, I like coconut mango more.'),\n",
       "  HumanMessage(content='Strawberry cheesecake is my favorite flavor'),\n",
       "  AIMessage(content='Well, I like coconut mango more.')]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It is still under the token limit, so run extra commands to see when it hits the limit.\n",
    "add_message(\"Strawberry cheesecake is my favorite flavor\", \"Well, I like coconut mango more.\")\n",
    "add_message(\"Strawberry cheesecake is my favorite flavor\", \"Well, I like coconut mango more.\")\n",
    "add_message(\"Strawberry cheesecake is my favorite flavor\", \"Well, I like coconut mango more.\")\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "#Observe that the memory generates SystemMessage to summarize the previous conversation when hits the max-token limit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
